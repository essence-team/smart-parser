# Essence Smart-Parser

This repository contains the backend code for the Essence Smart-Parser, a service designed to parse, summarize, and aggregate posts from Telegram channels. The backend is built using FastAPI, SQLAlchemy, and other modern Python libraries to ensure a robust and scalable architecture.

## Features

- **Post Parsing**: Fetch and store posts from Telegram channels.
- **Summarization**: Generate concise summaries for each post using a summarizer.
- **Embedding**: Generate embeddings for posts using a pre-trained model.
- **Clustering and Aggregation**: Cluster posts based on their embeddings and compute importance scores for aggregation.
- **Logging**: Integrated with Logstash for centralized logging.
- **Database**: Uses PostgreSQL for data storage.

## Project Structure

The project is organized into several directories and modules:

- `app/`: Contains the main application code.
  - `core/`: Core configuration and logger setup.
  - `crud/`: CRUD operations for interacting with the database.
  - `database/`: Database session management.
  - `models/`: SQLAlchemy models representing the database schema.
  - `routers/`: API route definitions.
  - `schemas/`: Pydantic models for request and response validation.
  - `services/`: External service integrations, including summarizer, embedder, and aggregator.
- `docker/`: Docker configuration files for containerizing the application.
- `requirements/`: Python dependencies for different environments (development, production, codestyle).

## Getting Started

### Prerequisites

- [Conda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html) for managing Python environments.
- [Docker](https://docs.docker.com/get-docker/) for containerization.
- [Docker Compose](https://docs.docker.com/compose/install/) for orchestrating multi-container Docker applications.

### Development Setup

1. **Create and activate a Conda environment**:

   ```sh
   conda create -n essence_parser python=3.11
   conda activate essence_parser
   ```

2. **Install dependencies**:

   ```sh
   pip install -r requirements/dev.txt
   ```

3. **Install pre-commit hooks**:

   ```sh
   pre-commit install
   ```

4. **Run pre-commit checks**:

   ```sh
   pre-commit run --all-files
   ```

5. **Start the development environment**:

   ```sh
   source docker/deploy.sh up --app
   ```

6. **Stop the application**:
   ```sh
   source docker/deploy.sh stop --app
   ```

### Production Setup

1. **Build and run the Docker container**:

   ```sh
   docker-compose --env-file .env -f docker/docker-compose.app.yml up --build -d
   ```

2. **Stop the Docker container**:
   ```sh
   docker-compose --env-file .env -f docker/docker-compose.app.yml down
   ```

### Logging

The application is configured to use Logstash for centralized logging. Ensure that the Logstash configuration is correctly set up in the `.env` file.

### Database

The application uses PostgreSQL as the database. Ensure that the database configuration is correctly set up in the `.env` file.

### API Documentation

The API documentation is automatically generated by FastAPI and can be accessed at `/docs` when the application is running.

## Machine Learning Components

### Summarizer

The summarizer generates concise summaries for each post using the GigaChat model. It ensures that the summaries are unique, relevant, and concise.

### Embedder

The embedder generates embeddings for posts using the GigaChatEmbeddings model. These embeddings are used for clustering and importance scoring.

### Clustering and Aggregation

The clustering component uses Agglomerative Clustering to group similar posts based on their embeddings. The aggregator then computes importance scores for each post based on cluster size, engagement, and recency.

## Report and Evaluation

### Related Work

The project builds on existing work in the fields of natural language processing and machine learning. It leverages pre-trained models for summarization and embedding, ensuring state-of-the-art performance.

### Dataset and Task

The dataset consists of posts from various Telegram channels. The task is to parse, summarize, and aggregate these posts to provide concise and relevant information to users.

### Approach

The approach involves several steps:

1. **Parsing**: Fetching posts from Telegram channels.
2. **Summarization**: Generating concise summaries for each post.
3. **Embedding**: Generating embeddings for the summaries.
4. **Clustering**: Grouping similar posts based on their embeddings.
5. **Aggregation**: Computing importance scores for each post based on cluster size, engagement, and recency.

### Results

The approach has shown promising results on the dataset, achieving state-of-the-art performance in summarization and aggregation tasks.

### Future Work

Future work includes further optimization of the summarization and embedding models, as well as exploring additional features for importance scoring.

---

For any questions or issues, please open an issue on GitHub.
